---
layout: blog
back: true
comments: True
flag: Server
background: gray
category: 后端
title: Redis 简介
date: 2019-07-08 21:38:00 GMT+0800 (CST)
background-image: /style/images/others/redis.jpg
tags:
- Server
---
# {{ page.title }}

**Redis(Remote Dictionary Service)** 即远程字典服务，是互联网技术领域使用最为广泛的存储中间件。Redis 以其超高的性能、完美的文档、简洁易懂的源码和丰富的客户端库支持在开源中间件领域广受好评。不同于之前文章就介绍的 Oracle 和 Mongodb，它是 key-value 型数据库，默认端口号为 6379。以下介绍基本[摘自掘进小册](https://juejin.im/book/5afc2e5f6fb9a07a9b362527/section/5afc2e5f51882542714ff291) 👍

> [网页在线体验戳这里](https://try.redis.io) 👈

## 基础类型

Redis 目前一共有五种基础类型:

| 基础类型        |   描述   |
| ------------ | ------- |
| 字符串 string | Redis 所有的数据结构都是以唯一的 key 字符串作为名称，然后通过这个唯一 key 值来获取相应的 value 数据 |
| 列表 list | 列表结构常用来做异步队列使用。将需要延后处理的任务结构体序列化成字符串塞进 Redis 的列表，另一个线程从这个列表中轮询数据进行处理 |
| 字典 hash | 内部实现结构上同 Java 的 HashMap 也是一致的，同样的数组 + 链表二维结构 |
| 集合 set | 相当于 Java 语言里面的 HashSet，它内部的键值对是无序的唯一的 |
| 有序集合 zset | 类似于 Java 的 SortedSet 和 HashMap 的结合体 |

### string / list / hash / set / zset

1、字符串 string

Redis 的字符串是**动态字符串**，是可以修改的字符串，内部结构实现上类似于 Java 的 ArrayList，采用预分配冗余空间的方式来减少内存的频繁分配，其最大长度为 512M。

```SHELL
# 键值对
> set name tate
OK
> get name
"tate"
> exists name
(integer) 1
> del name
(integer) 1
> get name
(nil)
```

```SHELL
# 批量键值对，节省网络耗时开销
> set name1 tate
OK
> set name2 holycoder
OK
> mget name1 name2 name3 # 返回一个列表
1) "tate"
2) "holycoder"
3) (nil)
> mset name1 boy name2 girl name3 unknown
> mget name1 name2 name3
1) "boy"
2) "girl"
3) "unknown"
```

还可以对 key 设置过期时间，到点自动删除，这个功能常用来控制缓存的失效时间:

```SHELL
> set name tate
> get name
"tate"
> expire name 5  # 5s 后过期
...  # wait for 5s
> get name
(nil)

> setex name 5 tate  # 5s 后过期，等价于 set+expire
> get name
"tate"
... # wait for 5s
> get name
(nil)

> setnx name tate  # 如果 name 不存在就执行 set 创建
(integer) 1
> get name
"tate"
> setnx name holycoder
(integer) 0  # 因为 name 已经存在，所以 set 创建不成功
> get name
"tate"  # 没有改变
```

2、列表 list

Redis 的列表相当于 Java 语言里面的 LinkedList，注意它是链表而不是数组。这意味着 list 的插入和删除操作非常快，时间复杂度为 O(1)，但是索引定位很慢，时间复杂度为 O(n):

![redis-list.gif](https://i.loli.net/2019/07/08/5d22fc3fd000195913.gif)

```SHELL
# 栈: 右边进右边出
> rpush books python java golang
(integer) 3
> rpop books
"golang"
> rpop books
"java"
> rpop books
"python"
> rpop books
(nil)
```

3、字典 hash

Redis 的字典相当于 Java 语言里面的 HashMap，它是无序字典。内部实现结构上同 Java 的 HashMap 也是一致的，同样的数组 + 链表二维结构。第一维 hash 的数组位置碰撞时，就会将碰撞的元素使用链表串接起来。不同的是，Redis 的字典的值只能是字符串，另外它们 **rehash** 的方式不一样，因为 Java 的 HashMap 在字典很大时，rehash 是个耗时的操作，需要一次性全部 rehash。Redis 为了高性能，不能堵塞服务，所以采用了渐进式 rehash 策略:

![redis-hash.gif](https://i.loli.net/2019/07/08/5d22fc3fcdcd761431.gif)

hash 结构也可以用来存储用户信息，不同于字符串一次性需要全部序列化整个对象，hash 可以对用户结构中的每个字段单独存储。这样当我们需要获取用户信息时可以进行部分获取。而以整个字符串的形式去保存用户信息的话就只能一次性全部读取，这样就会比较浪费网络流量。但 hash 结构的存储消耗要高于单个字符串，到底该使用 hash 还是字符串，需要根据实际情况再三权衡:

```SHELL
> hset books java "think in java"  # 命令行的字符串如果包含空格，要用引号括起来
(integer) 1
> hset books golang "concurrency in go"
(integer) 1
> hset books python "python cookbook"
(integer) 1
> hgetall books  # entries()，key 和 value 间隔出现
1) "java"
2) "think in java"
3) "golang"
4) "concurrency in go"
5) "python"
6) "python cookbook"
> hlen books
(integer) 3
> hget books java
"think in java"
> hset books golang "learning go programming"  # 因为是更新操作，所以返回 0
(integer) 0
> hget books golang
"learning go programming"
> hmset books java "effective java" python "learning python" golang "modern golang programming"  # 批量 set
OK
```

4、集合 set

Redis 的集合相当于 Java 语言里面的 HashSet，它内部的键值对是无序的唯一的。它的内部实现相当于一个特殊的字典，字典中所有的 value 都是一个值 NULL:

![redis-set.gif](https://i.loli.net/2019/07/08/5d22fd3445b4b30491.gif)

set 结构可以用来存储活动中奖的用户 ID，因为有去重功能，可以保证同一个用户不会中奖两次:

```SHELL
> sadd books python
(integer) 1
> sadd books python  #  重复
(integer) 0
> sadd books java golang
(integer) 2
> smembers books  # 注意顺序，和插入的并不一致，因为 set 是无序的
1) "java"
2) "python"
3) "golang"
> sismember books java  # 查询某个 value 是否存在，相当于 contains(o)
(integer) 1
> sismember books rust
(integer) 0
> scard books  # 获取长度相当于 count()
(integer) 3
> spop books  # 弹出一个
"java"
```

5、有序集合 zset

zset 类似于 Java 的 SortedSet 和 HashMap 的结合体，一方面它是一个 set，保证了内部 value 的唯一性，另一方面它可以给每个 value 赋予一个 score，代表这个 value 的排序权重。它的内部实现用的是一种叫做[**跳跃列表**](https://juejin.im/book/5afc2e5f6fb9a07a9b362527/section/5b5ac63d5188256255299d9c)的数据结构:

![redis-zset.gif](https://i.loli.net/2019/07/08/5d22fdec2d9a424961.gif)

例如 zset 可以用来存储学生的成绩，value 值是学生的 ID，score 是他的考试成绩。我们可以对成绩按分数进行排序就可以得到他的名次:

```SHELL
> zadd books 9.0 "think in java"
(integer) 1
> zadd books 8.9 "java concurrency"
(integer) 1
> zadd books 8.6 "java cookbook"
(integer) 1
> zrange books 0 -1  # 按 score 排序列出，参数区间为排名范围
1) "java cookbook"
2) "java concurrency"
3) "think in java"
> zrevrange books 0 -1  # 按 score 逆序列出，参数区间为排名范围
1) "think in java"
2) "java concurrency"
3) "java cookbook"
> zcard books  # 相当于 count()
(integer) 3
> zscore books "java concurrency"  # 获取指定 value 的 score
"8.9000000000000004"  # 内部 score 使用 double 类型进行存储，所以存在小数点精度问题
> zrank books "java concurrency"  # 排名
(integer) 1
> zrangebyscore books 0 8.91  # 根据分值区间遍历 zset
1) "java cookbook"
2) "java concurrency"
> zrangebyscore books -inf 8.91 withscores # 根据分值区间 (-∞, 8.91] 遍历 zset，同时返回分值。inf 代表 infinite，无穷大的意思。
1) "java cookbook"
2) "8.5999999999999996"
3) "java concurrency"
4) "8.9000000000000004"
> zrem books "java concurrency"  # 删除 value
(integer) 1
> zrange books 0 -1
1) "java cookbook"
2) "think in java"
```

以上这四种数据结构(除开字符串)是容器型数据结构，它们共享下面两条通用规则：

* **create if not exists**

如果容器不存在，那就创建一个，再进行操作。比如 rpush 操作刚开始是没有列表的，Redis 就会自动创建一个，然后再 rpush 进去新元素。

* **drop if no elements**

如果容器里元素没有了，那么立即删除元素，释放内存。这意味着 lpop 操作到最后一个元素，列表就消失了。

### 过期时间

Redis 所有的数据结构都可以设置过期时间，时间到了，Redis 会自动删除相应的对象。需要注意的是过期是以对象为单位，比如一个 hash 结构的过期是整个 hash 对象的过期，而不是其中的某个子 key。

还有一个需要特别注意的地方是如果一个字符串已经设置了过期时间，然后你调用了 set 方法修改了它，它的过期时间会消失:

```SHELL
> set tate yoyo
OK
> expire tate 600
(integer) 1
> ttl tate
(integer) 597
> set tate yoyo
OK
> ttl tate
(integer) -1
```

## 分布式锁

场景: 比如一个操作要修改用户的状态，修改状态需要先读出用户的状态，在内存里进行修改，改完了再存回去。如果这样的操作同时进行了，就会出现并发问题，因为读取和保存状态这两个操作不是**原子**的，即不会被线程调度机制打断的操作。这时候就会用到 Redis 的**分布式锁**。

分布式锁本质上要实现的目标就是在 Redis 里面占坑，一般是使用 **setnx**(set if not exists) 指令，只允许被一个客户端占坑。先来先占，用完了，再调用 del 指令释放茅坑。为了防止中间出现一些异常导出无法 del，一般会加个期限，过期后自动释放:

```SHELL
> setnx maokeng true
OK
> expire maokeng 5
... do something critical ...
> del maokeng
(integer) 1
```

但是如果在 setnx 和 expire 之间服务器进程突然挂掉了，无法调用 expire，则还是成了死锁。因此 Redis 2.8 版本中作者加入了 set 指令的扩展参数，使得 setnx 和 expire 指令可以一起执行:

```SHELL
# 原子指令
> set maokeng true ex 5 nx
OK
... do something critical ...
> del maokeng
```

伴随而来的还有一个问题，就是超时。如果在加锁和释放锁之间的逻辑执行的太长，以至于超出了锁的超时限制，就会出现问题。因为这时候第一个线程持有的锁过期了，临界区的逻辑还没有执行完，这个时候第二个线程就提前重新持有了这把锁，导致临界区代码不能得到严格的串行执行:

```PY
# 验证随机数是否一致
tag = random.nextint()  # 随机数
if redis.set(key, tag, nx=True, ex=5):
  do_something()
  redis.delifequals(key, tag)  # 假想的 delifequals 指令
```

匹配 value 和删除 key 不是一个原子操作，Redis 也没有提供类似于 delifequals 这样的指令。目前可以使用 [**Lua 脚本**](https://zh.wikipedia.org/wiki/Lua)来处理，因为 Lua 脚本可以保证连续多个指令的原子性执行，但也只是说相对更安全一些:

```SHELL
# delifequals
if redis.call("get",KEYS[1]) == ARGV[1] then
  return redis.call("del",KEYS[1])
else
  return 0
end
```

另外提到一下**可重入性**，指线程在持有锁的情况下再次请求加锁，如果一个锁支持同一个线程的多次加锁，那么这个锁就是可重入的，这里就不展开讨论了。

## 延时队列

首先我们了解下什么是[**消息队列(Message Queue)**](https://github.com/jasonGeng88/blog/blob/master/201705/MQ.md)，即 **MQ**。它是一种应用间的通信方式，消息发送后可以立即返回，由消息系统来确保消息的可靠传递。消息发布者只管把消息发布到 MQ 中而不用管谁来取，消息使用者只管从 MQ 中取消息而不管是谁发布的。这样发布者和使用者都不用知道对方的存在。市面上常用的有 [**RabbitMQ**](https://www.rabbitmq.com) 和 [**Kafka**](https://kafka.apache.org) 等，来给应用程序之间增加异步消息传递功能。

Redis 的消息队列不是专业的消息队列，使用起来比较简单，它没有非常多的高级特性，没有 ack 保证，如果对消息的可靠性有着极致的追求，那么它就不适合使用。Redis 的 list(列表) 数据结构常用来作为异步消息队列使用，使用 `rpush/lpush` 操作入队列，使用 `lpop/rpop` 来出队列。

客户端是通过队列的 pop 操作来获取消息，然后进行处理。处理完了再接着获取消息，再进行处理。如此循环往复，这便是作为队列消费者的客户端的生命周期。可是如果队列空了，客户端就会陷入 pop 的死循环，可能拉高客户端的 CPU 占用和 Redis 的 QPS，解决方案之一就是通过 sleep 指令让线程"睡一会儿":

```SHELL
time.sleep(1)  # python 睡 1s
Thread.sleep(1000)  # java 睡 1s
```

上述又会导致另一个问题，即消息的延迟增大，因此采用 `blpop/brpop` 替代前面的 `lpop/rpop`，就完美解决了上面的问题。这两个指令的前缀字符 b 代表的是 `blocking`，也就是**阻塞读**。阻塞读在队列没有数据的时候，会立即进入休眠状态，一旦数据到来，则立刻醒过来。消息的延迟几乎为零。

那么问题又来了，如果线程一直阻塞在哪里，Redis 的客户端连接就成了闲置连接，闲置过久，服务器一般会主动断开连接，减少闲置资源占用。这个时候 `blpop/brpop` 会抛出异常来，所以要注意捕获异常。

上面有一部分讲到了分布式锁的问题，但是没有提到客户端在处理请求时加锁没加成功怎么办。一般有 3 种策略来处理加锁失败：

* 直接抛出异常，通知用户稍后重试；
* sleep 一会再重试；
* 将请求转移至延时队列，过一会再试；

**延时队列**可以通过 Redis 的 zset(有序列表) 来实现。我们将消息序列化成一个字符串作为 zset 的 value，这个消息的到期处理时间作为score，然后用多个线程轮询 zset 获取到期的任务进行处理，多个线程是为了保障可用性，万一挂了一个线程还有其它线程可以继续处理。因为有多个线程，所以需要考虑并发争抢任务，确保任务不能被多次执行:

```PY
def delay(msg):
  msg.id = str(uuid.uuid4())  # 保证 value 值唯一
  value = json.dumps(msg)
  retry_ts = time.time() + 5  # 5 秒后重试
  redis.zadd("delay-queue", retry_ts, value)


def loop():
  while True:
    # 最多取 1 条
    values = redis.zrangebyscore("delay-queue", 0, time.time(), start=0, num=1)
    if not values:
      time.sleep(1)  # 延时队列空的，休息 1s
      continue
    value = values[0]  # 拿第一条，也只有一条
    success = redis.zrem("delay-queue", value)  # 从消息队列中移除该消息
    if success:  # 因为有多进程并发的可能，最终只会有一个进程可以抢到消息
      msg = json.loads(value)
      handle_msg(msg)
```

Redis 的 zrem 方法是多线程多进程争抢任务的关键，它的返回值决定了当前实例有没有抢到任务，因为 loop 方法可能会被多个线程、多个进程调用，同一个任务可能会被多个进程线程抢到，通过 zrem 来决定唯一的属主。同时，我们要注意一定要对 handle_msg 进行异常捕获，避免因为个别任务处理问题导致循环异常退出。

## HyperLogLog 统计

场景:如果要负责维护一个网站，要记录 PV 和 UV。PV 那非常好办，给每个网页一个独立的 Redis 计数器就可以了，这个计数器的 key 后缀加上当天的日期。这样来一个请求，incrby 一次，最终就可以统计出所有的 PV 数据。

UV 因为要考虑去重，一般想法是为每一个页面一个独立的 set 集合来存储所有当天访问过此页面的用户 ID。当一个请求过来时，我们使用 sadd 将用户 ID 塞进去就可以了。通过 scard 可以取出这个集合的大小，这个数字就是这个页面的 UV 数据。但是，如果你的页面访问量非常大，比如一个爆款页面几千万的 UV，你需要一个很大的 set 集合来统计，这就非常浪费空间。因此 **HyperLogLog** 就派上用场了。

Redis 提供了 HyperLogLog 数据结构就是用来解决这种统计问题的。HyperLogLog 提供不精确的去重计数方案，虽然不精确但是也不是非常不精确，标准误差是 0.81%。它提供了以下几个常用指令:

* **pfadd** - 增加计数
* **pfcount** - 获取计数
* **pfmerge** - 合并计数，用于将多个 pf 计数值累加在一起形成一个新的 pf 值

```SHELL
> pfadd tate user1
(integer) 1
> pfcount tate
(integer) 1
> pfadd tate user2 user3 user4 user5
(integer) 1
> pfcount tate
(integer) 5
```

> pfadd 中的 `pf` 是 HyperLogLog 数据结构发明人 Philippe Flajolet 的首字母缩写。

## 布隆过滤器

场景:我们在使用新闻客户端看新闻时，它会给我们不停地推荐新的内容，它每次推荐时要去重，去掉那些已经看过的内容。问题来了，新闻客户端推荐系统如何实现推送去重的?

如果历史记录存储在关系数据库里，去重就需要频繁地对数据库进行 exists 查询，当系统并发量很高时，数据库是很难扛住压力的。可能又想到了缓存，但是如此多的历史记录全部缓存起来，那得浪费多大存储空间呢？而且这个存储空间是随着时间线性增长。这时就需要用到**布隆过滤器(Bloom Filter)**了。它就是专门用来解决这种去重问题的。它在起到去重的同时，在空间上还能节省 90% 以上，只是稍微有那么点不精确，也就是有一定的误判概率。

布隆过滤器可以理解为一个不怎么精确的 set 结构，当你使用它的 `contains` 方法判断某个对象是否存在时，它可能会误判：当布隆过滤器说某个值存在时，这个值可能不存在；当它说不存在时，那就肯定不存在。Redis 官方提供的布隆过滤器到了 Redis 4.0 提供了插件功能之后才正式登场。布隆过滤器作为一个插件加载到 Redis Server 中，给 Redis 提供了强大的布隆去重功能，其基本命令如下:

* **bf.add** - 添加元素
* **bf.exists** - 查询元素是否存在
* **bf.madd** - 批量添加元素
* **bf.mexists** - 批量查询元素是否存在

```SHELL
> bf.add codehole user1
(integer) 1
> bf.add codehole user2
(integer) 1
> bf.add codehole user3
(integer) 1
> bf.exists codehole user1
(integer) 1
> bf.exists codehole user2
(integer) 1
> bf.exists codehole user3
(integer) 1
> bf.exists codehole user4
(integer) 0
> bf.madd codehole user4 user5 user6
1) (integer) 1
2) (integer) 1
3) (integer) 1
> bf.mexists codehole user4 user5 user6 user7
1) (integer) 1
2) (integer) 1
3) (integer) 1
4) (integer) 0
```

## 漏斗限流

场景:用户的发帖、回复、点赞等行为都要严格受控，一般要严格限定某行为在规定时间内允许的次数，超过了次数那就是非法行为。对非法行为，业务必须规定适当的惩处策略，因此除了控制流量，限流还有一个应用目的是用于控制用户行为，避免垃圾请求。

**漏斗限流(funnel)**是最常用的限流方法之一，顾名思义，这个算法的灵感源于漏斗（funnel）的结构。漏斗的剩余空间就代表着当前行为可以持续进行的数量，漏嘴的流水速率代表着系统允许该行为的最大频率。Redis 4.0 提供了一个限流 Redis 模块，它叫 **redis-cell**。该模块也使用了漏斗算法，并提供了原子的限流指令。该模块只有一条指令 `cl.throttle`:

```TEXT
> cl.throttle tate:reply 15 30 60 1
                   ▲     ▲  ▲  ▲  ▲
                   |     |  |  |  └───── need 1 quota (可选参数，默认值也是1)
                   |     |  └──┴─────── 30 operations / 60 seconds 这是漏水速率
                   |     └───────────── 15 capacity 这是漏斗容量
                   └─────────────────── key tate
```

> To be continued

## 参考链接

1. [Redis 深度历险：核心原理与应用实践 - 掘进小册](https://juejin.im/book/5afc2e5f6fb9a07a9b362527/section/5afc2e5f51882542714ff291) By 老钱
2. [基于 Redis 的分布式锁到底安全吗？](http://zhangtielei.com/posts/blog-redlock-reasoning.html)
